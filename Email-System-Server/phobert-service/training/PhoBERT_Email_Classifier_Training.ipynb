{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ PhoBERT Email Classifier Training\n",
        "\n",
        "**VeryGoodMail - Email Classification System**\n",
        "\n",
        "Notebook n√†y h∆∞·ªõng d·∫´n c√°ch train PhoBERT ƒë·ªÉ:\n",
        "1. **Spam Detection**: Ph√¢n lo·∫°i email spam/ham\n",
        "2. **Sentiment Analysis**: Ph√¢n t√≠ch c·∫£m x√∫c email (t√≠ch c·ª±c/ti√™u c·ª±c/trung l·∫≠p)\n",
        "3. **Email Categorization**: Ph√¢n lo·∫°i v√†o th∆∞ m·ª•c (Quan tr·ªçng, X√£ h·ªôi, Khuy·∫øn m√£i, C·∫≠p nh·∫≠t)\n",
        "\n",
        "---\n",
        "¬© 2025 VeryGoodMail by **Ho√†n**"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìã B∆∞·ªõc 1: C√†i ƒë·∫∑t th∆∞ vi·ªán"
      ],
      "metadata": {
        "id": "step1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets torch accelerate sentencepiece\n",
        "!pip install -q underthesea  # Vietnamese NLP toolkit\n",
        "!pip install -q scikit-learn pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# Check GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä B∆∞·ªõc 2: Chu·∫©n b·ªã d·ªØ li·ªáu training\n",
        "\n",
        "### T√πy ch·ªçn A: S·ª≠ d·ª•ng d·ªØ li·ªáu m·∫´u (demo)\n",
        "### T√πy ch·ªçn B: Upload d·ªØ li·ªáu c·ªßa b·∫°n"
      ],
      "metadata": {
        "id": "step2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# D·ªØ li·ªáu m·∫´u cho demo - Trong th·ª±c t·∫ø c·∫ßn nhi·ªÅu d·ªØ li·ªáu h∆°n!\n",
        "# ============================================================\n",
        "\n",
        "# Spam Detection Dataset\n",
        "spam_data = {\n",
        "    'text': [\n",
        "        # Spam samples (Vietnamese)\n",
        "        \"Ch√∫c m·ª´ng! B·∫°n ƒë√£ tr√∫ng th∆∞·ªüng 1 t·ª∑ ƒë·ªìng. Nh·∫•n v√†o ƒë√¢y ƒë·ªÉ nh·∫≠n ngay\",\n",
        "        \"Ki·∫øm ti·ªÅn online d·ªÖ d√†ng, thu nh·∫≠p 50 tri·ªáu/th√°ng kh√¥ng c·∫ßn v·ªën\",\n",
        "        \"Gi·∫£m c√¢n nhanh ch√≥ng 10kg trong 1 tu·∫ßn, cam k·∫øt hi·ªáu qu·∫£ 100%\",\n",
        "        \"Khuy·∫øn m√£i s·ªëc! Ch·ªâ h√¥m nay, mua 1 t·∫∑ng 10, nhanh tay ƒë·∫∑t h√†ng\",\n",
        "        \"B·∫°n ƒë∆∞·ª£c ch·ªçn ng·∫´u nhi√™n ƒë·ªÉ nh·∫≠n iPhone 15 Pro Max mi·ªÖn ph√≠\",\n",
        "        \"Vay ti·ªÅn online kh√¥ng c·∫ßn th·∫ø ch·∫•p, duy·ªát nhanh trong 5 ph√∫t\",\n",
        "        \"Thu·ªëc tƒÉng c∆∞·ªùng sinh l√Ω nam gi·ªõi, hi·ªáu qu·∫£ sau 1 l·∫ßn d√πng\",\n",
        "        \"Click ngay ƒë·ªÉ nh·∫≠n 500.000ƒë v√†o t√†i kho·∫£n c·ªßa b·∫°n\",\n",
        "        \"B·∫°n c√≥ kho·∫£n vay ch∆∞a thanh to√°n, li√™n h·ªá ngay ƒë·ªÉ tr√°nh b·ªã kh√≥a\",\n",
        "        \"Ch∆∞∆°ng tr√¨nh kh√°ch h√†ng th√¢n thi·∫øt, b·∫°n nh·∫≠n ƒë∆∞·ª£c 10 tri·ªáu ƒëi·ªÉm th∆∞·ªüng\",\n",
        "        \n",
        "        # Spam samples (English)\n",
        "        \"Congratulations! You have won $1,000,000 in our lottery\",\n",
        "        \"Make money fast from home, earn $10,000 per week\",\n",
        "        \"Click here to claim your free iPhone 15 now\",\n",
        "        \"Nigerian prince needs your help to transfer $50 million\",\n",
        "        \"Hot singles in your area want to meet you tonight\",\n",
        "        \"Buy cheap medications online without prescription\",\n",
        "        \"Your account has been compromised, click to verify immediately\",\n",
        "        \"Limited time offer! 90% discount on all luxury watches\",\n",
        "        \"You are selected for exclusive investment opportunity\",\n",
        "        \"Work from home and earn passive income easily\",\n",
        "        \n",
        "        # Ham samples (Vietnamese)\n",
        "        \"Anh ∆°i, cu·ªôc h·ªçp ng√†y mai l√∫c 9h s√°ng t·∫°i ph√≤ng h·ªçp A nh√©\",\n",
        "        \"Em g·ª≠i anh b√°o c√°o th√°ng 11, anh review gi√∫p em v·ªõi ·∫°\",\n",
        "        \"C·∫£m ∆°n anh ƒë√£ h·ªó tr·ª£ d·ª± √°n, em r·∫•t tr√¢n tr·ªçng\",\n",
        "        \"Nh·∫Øc l·ªãch: Deadline n·ªôp b√†i v√†o th·ª© 6 tu·∫ßn n√†y\",\n",
        "        \"ƒê√¢y l√† bi√™n b·∫£n cu·ªôc h·ªçp h√¥m qua, m·ªùi m·ªçi ng∆∞·ªùi xem\",\n",
        "        \"Anh cho em h·ªèi v·ªÅ ti·∫øn ƒë·ªô d·ª± √°n XYZ ƒë∆∞·ª£c kh√¥ng ·∫°?\",\n",
        "        \"X√°c nh·∫≠n: Em ƒë√£ nh·∫≠n ƒë∆∞·ª£c t√†i li·ªáu anh g·ª≠i\",\n",
        "        \"M·ªùi anh/ch·ªã tham d·ª± bu·ªïi workshop v√†o th·ª© 3 tu·∫ßn sau\",\n",
        "        \"G·ª≠i anh b·∫£ng ch·∫•m c√¥ng th√°ng 11, anh k√Ω duy·ªát gi√∫p em\",\n",
        "        \"Team m√¨nh h·ªçp standup l√∫c 10h s√°ng mai nh√©\",\n",
        "        \n",
        "        # Ham samples (English)\n",
        "        \"Meeting scheduled for tomorrow at 10 AM in room 301\",\n",
        "        \"Please review the attached quarterly report\",\n",
        "        \"Thank you for your help with the project presentation\",\n",
        "        \"Reminder: Deadline for submission is this Friday\",\n",
        "        \"Here are the meeting notes from yesterday's discussion\",\n",
        "        \"Can you provide an update on the project status?\",\n",
        "        \"Confirming receipt of the documents you sent\",\n",
        "        \"You are invited to the workshop next Tuesday\",\n",
        "        \"Monthly timesheet attached for your approval\",\n",
        "        \"Team standup tomorrow at 9 AM as usual\",\n",
        "    ],\n",
        "    'label': [1]*10 + [1]*10 + [0]*10 + [0]*10  # 1 = spam, 0 = ham\n",
        "}\n",
        "\n",
        "# Sentiment Dataset\n",
        "sentiment_data = {\n",
        "    'text': [\n",
        "        # Positive (Vietnamese)\n",
        "        \"C·∫£m ∆°n anh r·∫•t nhi·ªÅu, d·ªãch v·ª• tuy·ªát v·ªùi!\",\n",
        "        \"Em r·∫•t h√†i l√≤ng v·ªõi s·∫£n ph·∫©m n√†y, ch·∫•t l∆∞·ª£ng xu·∫•t s·∫Øc\",\n",
        "        \"ƒê·ªôi ng≈© h·ªó tr·ª£ r·∫•t nhi·ªát t√¨nh v√† chuy√™n nghi·ªáp\",\n",
        "        \"Tuy·ªát v·ªùi! ƒê√∫ng nh∆∞ mong ƒë·ª£i, s·∫Ω ·ªßng h·ªô ti·∫øp\",\n",
        "        \"Giao h√†ng nhanh, ƒë√≥ng g√≥i c·∫©n th·∫≠n, r·∫•t ƒë·∫πp\",\n",
        "        \n",
        "        # Positive (English)\n",
        "        \"Thank you so much, excellent service!\",\n",
        "        \"Very satisfied with this product, outstanding quality\",\n",
        "        \"The support team is very helpful and professional\",\n",
        "        \"Wonderful! Exactly as expected, will continue to support\",\n",
        "        \"Fast delivery, careful packaging, very nice\",\n",
        "        \n",
        "        # Negative (Vietnamese)\n",
        "        \"Th·∫•t v·ªçng qu√°, s·∫£n ph·∫©m k√©m ch·∫•t l∆∞·ª£ng\",\n",
        "        \"D·ªãch v·ª• t·ªá, nh√¢n vi√™n thi·∫øu chuy√™n nghi·ªáp\",\n",
        "        \"ƒê·ª£i m√£i kh√¥ng th·∫•y ph·∫£n h·ªìi, r·∫•t b·ª±c m√¨nh\",\n",
        "        \"H√†ng giao sai, kh√¥ng ƒë√∫ng m√¥ t·∫£, y√™u c·∫ßu ho√†n ti·ªÅn\",\n",
        "        \"Qu√° t·ªá! Kh√¥ng bao gi·ªù mua ·ªü ƒë√¢y n·ªØa\",\n",
        "        \n",
        "        # Negative (English)\n",
        "        \"Very disappointed, poor quality product\",\n",
        "        \"Terrible service, unprofessional staff\",\n",
        "        \"Been waiting forever, no response, very frustrated\",\n",
        "        \"Wrong item delivered, not as described, requesting refund\",\n",
        "        \"Awful! Never buying from here again\",\n",
        "        \n",
        "        # Neutral (Vietnamese)\n",
        "        \"Em x√°c nh·∫≠n ƒë√£ nh·∫≠n ƒë∆∞·ª£c email c·ªßa anh\",\n",
        "        \"ƒê√¢y l√† b√°o c√°o tu·∫ßn n√†y, m·ªùi anh xem\",\n",
        "        \"Th√¥ng b√°o: H·ªá th·ªëng b·∫£o tr√¨ v√†o 10h t·ªëi nay\",\n",
        "        \"G·ª≠i anh file ƒë√≠nh k√®m theo y√™u c·∫ßu\",\n",
        "        \"Nh·∫Øc l·ªãch: Cu·ªôc h·ªçp l√∫c 2h chi·ªÅu mai\",\n",
        "        \n",
        "        # Neutral (English)\n",
        "        \"Confirming receipt of your email\",\n",
        "        \"Here is this week's report for your review\",\n",
        "        \"Notice: System maintenance at 10 PM tonight\",\n",
        "        \"Attached file as requested\",\n",
        "        \"Reminder: Meeting at 2 PM tomorrow\",\n",
        "    ],\n",
        "    'label': [2]*5 + [2]*5 + [0]*5 + [0]*5 + [1]*5 + [1]*5  # 0=negative, 1=neutral, 2=positive\n",
        "}\n",
        "\n",
        "# Category Dataset\n",
        "category_data = {\n",
        "    'text': [\n",
        "        # Important\n",
        "        \"KH·∫®N C·∫§P: C·∫ßn x·ª≠ l√Ω ngay tr∆∞·ªõc 5h chi·ªÅu h√¥m nay\",\n",
        "        \"Quan tr·ªçng: Y√™u c·∫ßu ph√™ duy·ªát g·∫•p t·ª´ CEO\",\n",
        "        \"URGENT: Action required before end of day\",\n",
        "        \"Important: Your password will expire in 24 hours\",\n",
        "        \"C·∫£nh b√°o b·∫£o m·∫≠t: Ph√°t hi·ªán ƒëƒÉng nh·∫≠p b·∫•t th∆∞·ªùng\",\n",
        "        \n",
        "        # Social\n",
        "        \"Nguy·ªÖn VƒÉn A ƒë√£ th√≠ch b√†i vi·∫øt c·ªßa b·∫°n\",\n",
        "        \"B·∫°n c√≥ 5 y√™u c·∫ßu k·∫øt b·∫°n m·ªõi\",\n",
        "        \"John commented on your photo\",\n",
        "        \"You have 3 new followers on Instagram\",\n",
        "        \"Nh·∫Øc nh·ªü: Sinh nh·∫≠t c·ªßa Minh v√†o ng√†y mai\",\n",
        "        \n",
        "        # Promotions\n",
        "        \"Gi·∫£m gi√° 50% t·∫•t c·∫£ s·∫£n ph·∫©m m√πa h√®\",\n",
        "        \"Flash sale: Ch·ªâ 24 gi·ªù, gi·∫£m ƒë·∫øn 70%\",\n",
        "        \"Summer sale: 50% off all items\",\n",
        "        \"New collection available now - Shop today!\",\n",
        "        \"∆Øu ƒë√£i ƒë·ªôc quy·ªÅn d√†nh ri√™ng cho b·∫°n\",\n",
        "        \n",
        "        # Updates\n",
        "        \"ƒê∆°n h√†ng #12345 c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c giao th√†nh c√¥ng\",\n",
        "        \"C·∫≠p nh·∫≠t: G√≥i h√†ng ƒëang tr√™n ƒë∆∞·ªùng giao\",\n",
        "        \"Your order has been shipped\",\n",
        "        \"Software update available for your device\",\n",
        "        \"Sao k√™ t√†i kho·∫£n th√°ng 11 ƒë√£ s·∫µn s√†ng\",\n",
        "        \n",
        "        # Primary (general)\n",
        "        \"Cu·ªôc h·ªçp team v√†o th·ª© 2 tu·∫ßn sau\",\n",
        "        \"G·ª≠i anh b√°o c√°o d·ª± √°n nh∆∞ ƒë√£ trao ƒë·ªïi\",\n",
        "        \"Team meeting scheduled for next Monday\",\n",
        "        \"Project update as discussed yesterday\",\n",
        "        \"Xin ch√†o, em mu·ªën h·ªèi v·ªÅ v·∫•n ƒë·ªÅ...\",\n",
        "    ],\n",
        "    'label': [0]*5 + [1]*5 + [2]*5 + [3]*5 + [4]*5  # 0=important, 1=social, 2=promotions, 3=updates, 4=primary\n",
        "}\n",
        "\n",
        "print(f\"Spam dataset: {len(spam_data['text'])} samples\")\n",
        "print(f\"Sentiment dataset: {len(sentiment_data['text'])} samples\")\n",
        "print(f\"Category dataset: {len(category_data['text'])} samples\")"
      ],
      "metadata": {
        "id": "sample_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß B∆∞·ªõc 3: T·∫°o Dataset class v√† load PhoBERT"
      ],
      "metadata": {
        "id": "step3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmailDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for email classification.\"\"\"\n",
        "    \n",
        "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "        \n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Load PhoBERT tokenizer\n",
        "print(\"Loading PhoBERT tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
        "print(\"Tokenizer loaded successfully!\")"
      ],
      "metadata": {
        "id": "dataset_class"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üèãÔ∏è B∆∞·ªõc 4: Training Functions"
      ],
      "metadata": {
        "id": "step4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    \"\"\"Compute metrics for evaluation.\"\"\"\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    \n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    \n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "def train_classifier(train_data, num_labels, model_name, output_dir, epochs=5):\n",
        "    \"\"\"\n",
        "    Train a PhoBERT classifier.\n",
        "    \n",
        "    Args:\n",
        "        train_data: dict with 'text' and 'label' keys\n",
        "        num_labels: number of classification labels\n",
        "        model_name: name for saving the model\n",
        "        output_dir: directory to save the model\n",
        "        epochs: number of training epochs\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Training: {model_name}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    # Split data\n",
        "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "        train_data['text'], train_data['label'],\n",
        "        test_size=0.2, random_state=SEED, stratify=train_data['label']\n",
        "    )\n",
        "    \n",
        "    print(f\"Train samples: {len(train_texts)}\")\n",
        "    print(f\"Validation samples: {len(val_texts)}\")\n",
        "    \n",
        "    # Create datasets\n",
        "    train_dataset = EmailDataset(train_texts, train_labels, tokenizer)\n",
        "    val_dataset = EmailDataset(val_texts, val_labels, tokenizer)\n",
        "    \n",
        "    # Load model\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        \"vinai/phobert-base\",\n",
        "        num_labels=num_labels\n",
        "    )\n",
        "    \n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./{output_dir}\",\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        warmup_steps=100,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "        fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
        "    )\n",
        "    \n",
        "    # Initialize Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "    )\n",
        "    \n",
        "    # Train\n",
        "    print(\"\\nStarting training...\")\n",
        "    trainer.train()\n",
        "    \n",
        "    # Evaluate\n",
        "    print(\"\\nEvaluation results:\")\n",
        "    eval_results = trainer.evaluate()\n",
        "    for key, value in eval_results.items():\n",
        "        print(f\"  {key}: {value:.4f}\")\n",
        "    \n",
        "    # Save model\n",
        "    final_path = f\"./models/{model_name}\"\n",
        "    trainer.save_model(final_path)\n",
        "    tokenizer.save_pretrained(final_path)\n",
        "    print(f\"\\nModel saved to: {final_path}\")\n",
        "    \n",
        "    return trainer, eval_results"
      ],
      "metadata": {
        "id": "training_functions"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ B∆∞·ªõc 5: Train c√°c models"
      ],
      "metadata": {
        "id": "step5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create models directory\n",
        "os.makedirs('./models', exist_ok=True)\n",
        "\n",
        "# Train Spam Classifier\n",
        "spam_trainer, spam_results = train_classifier(\n",
        "    train_data=spam_data,\n",
        "    num_labels=2,\n",
        "    model_name=\"spam_classifier\",\n",
        "    output_dir=\"spam_output\",\n",
        "    epochs=5\n",
        ")"
      ],
      "metadata": {
        "id": "train_spam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Sentiment Classifier\n",
        "sentiment_trainer, sentiment_results = train_classifier(\n",
        "    train_data=sentiment_data,\n",
        "    num_labels=3,\n",
        "    model_name=\"sentiment_classifier\",\n",
        "    output_dir=\"sentiment_output\",\n",
        "    epochs=5\n",
        ")"
      ],
      "metadata": {
        "id": "train_sentiment"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Category Classifier\n",
        "category_trainer, category_results = train_classifier(\n",
        "    train_data=category_data,\n",
        "    num_labels=5,\n",
        "    model_name=\"category_classifier\",\n",
        "    output_dir=\"category_output\",\n",
        "    epochs=5\n",
        ")"
      ],
      "metadata": {
        "id": "train_category"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ B∆∞·ªõc 6: Test models"
      ],
      "metadata": {
        "id": "step6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_classifier(model_path, test_texts, label_names):\n",
        "    \"\"\"Test a trained classifier with sample texts.\"\"\"\n",
        "    print(f\"\\nTesting model: {model_path}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Load model\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    for text in test_texts:\n",
        "        inputs = tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=256,\n",
        "            return_tensors='pt'\n",
        "        ).to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            probs = torch.softmax(outputs.logits, dim=1)\n",
        "            pred_idx = probs.argmax().item()\n",
        "            confidence = probs[0][pred_idx].item()\n",
        "        \n",
        "        print(f\"\\nText: {text[:80]}...\" if len(text) > 80 else f\"\\nText: {text}\")\n",
        "        print(f\"Prediction: {label_names[pred_idx]} ({confidence:.2%})\")\n",
        "\n",
        "# Test Spam Classifier\n",
        "spam_test_texts = [\n",
        "    \"B·∫°n ƒë√£ tr√∫ng th∆∞·ªüng 100 tri·ªáu ƒë·ªìng! Click ngay ƒë·ªÉ nh·∫≠n\",\n",
        "    \"Cu·ªôc h·ªçp team v√†o 9h s√°ng mai t·∫°i ph√≤ng h·ªçp A\",\n",
        "    \"URGENT: You have won a free cruise vacation!\",\n",
        "    \"Please review the attached quarterly report\"\n",
        "]\n",
        "test_classifier(\"./models/spam_classifier\", spam_test_texts, [\"Ham\", \"Spam\"])\n",
        "\n",
        "# Test Sentiment Classifier\n",
        "sentiment_test_texts = [\n",
        "    \"C·∫£m ∆°n anh r·∫•t nhi·ªÅu, d·ªãch v·ª• tuy·ªát v·ªùi!\",\n",
        "    \"Th·∫•t v·ªçng qu√°, s·∫£n ph·∫©m k√©m ch·∫•t l∆∞·ª£ng\",\n",
        "    \"ƒê√¢y l√† b√°o c√°o tu·∫ßn n√†y, m·ªùi anh xem\",\n",
        "]\n",
        "test_classifier(\"./models/sentiment_classifier\", sentiment_test_texts, [\"Negative\", \"Neutral\", \"Positive\"])"
      ],
      "metadata": {
        "id": "test_models"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ B∆∞·ªõc 7: Export models"
      ],
      "metadata": {
        "id": "step7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip models for download\n",
        "!zip -r phobert_email_models.zip ./models/\n",
        "\n",
        "# Download link (in Colab)\n",
        "from google.colab import files\n",
        "files.download('phobert_email_models.zip')"
      ],
      "metadata": {
        "id": "export"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìñ H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng models\n",
        "\n",
        "### 1. Gi·∫£i n√©n models\n",
        "```bash\n",
        "unzip phobert_email_models.zip -d /path/to/phobert-service/\n",
        "```\n",
        "\n",
        "### 2. Ch·∫°y PhoBERT service\n",
        "```bash\n",
        "cd Email-System-Server/phobert-service\n",
        "pip install -r requirements.txt\n",
        "USE_PHOBERT=true python main.py\n",
        "```\n",
        "\n",
        "### 3. Test API\n",
        "```bash\n",
        "curl -X POST http://localhost:8000/classify \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -d '{\"subject\": \"Test email\", \"body\": \"Hello world\"}'\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Tips ƒë·ªÉ c·∫£i thi·ªán accuracy\n",
        "\n",
        "1. **Th√™m nhi·ªÅu d·ªØ li·ªáu**: Dataset hi·ªán t·∫°i ch·ªâ c√≥ ~30-40 samples m·ªói lo·∫°i. C·∫ßn √≠t nh·∫•t 1000+ samples ƒë·ªÉ c√≥ k·∫øt qu·∫£ t·ªët.\n",
        "\n",
        "2. **Data augmentation**: S·ª≠ d·ª•ng c√°c k·ªπ thu·∫≠t nh∆∞:\n",
        "   - Back-translation (d·ªãch sang ng√¥n ng·ªØ kh√°c r·ªìi d·ªãch l·∫°i)\n",
        "   - Synonym replacement\n",
        "   - Random insertion/deletion\n",
        "\n",
        "3. **Hyperparameter tuning**: Th·ª≠ c√°c gi√° tr·ªã kh√°c nhau cho learning rate, batch size, epochs.\n",
        "\n",
        "4. **Cross-validation**: S·ª≠ d·ª•ng k-fold cross-validation ƒë·ªÉ ƒë√°nh gi√° model ch√≠nh x√°c h∆°n.\n",
        "\n",
        "5. **Ensemble**: K·∫øt h·ª£p nhi·ªÅu models ƒë·ªÉ c√≥ k·∫øt qu·∫£ t·ªët h∆°n.\n",
        "\n",
        "---\n",
        "\n",
        "¬© 2025 VeryGoodMail by **Ho√†n**"
      ],
      "metadata": {
        "id": "instructions"
      }
    }
  ]
}
