{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üáªüá≥ VeryGoodMail - PhoBERT Email Classification Training\n",
        "\n",
        "Notebook n√†y gi√∫p b·∫°n train c√°c model PhoBERT cho:\n",
        "- **Spam Detection**: Ph√°t hi·ªán email spam\n",
        "- **Sentiment Analysis**: Ph√¢n t√≠ch c·∫£m x√∫c \n",
        "- **Category Classification**: Ph√¢n lo·∫°i email\n",
        "\n",
        "¬© 2025 VeryGoodMail by Ho√†n"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup Environment"
      ],
      "metadata": {
        "id": "setup"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install transformers torch datasets scikit-learn pandas underthesea -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from transformers import (\n",
        "    AutoTokenizer, \n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from datasets import Dataset\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check GPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using device: {device}')\n",
        "if device == 'cuda':\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load PhoBERT Tokenizer"
      ],
      "metadata": {
        "id": "tokenizer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load PhoBERT tokenizer\n",
        "model_name = \"vinai/phobert-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "print(f'Loaded tokenizer: {model_name}')"
      ],
      "metadata": {
        "id": "load_tokenizer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Prepare Dataset\n",
        "\n",
        "Upload dataset c·ªßa b·∫°n ho·∫∑c s·ª≠ d·ª•ng sample data"
      ],
      "metadata": {
        "id": "data"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data - Thay th·∫ø b·∫±ng dataset c·ªßa b·∫°n\n",
        "# Format: text, label\n",
        "\n",
        "# Spam detection dataset\n",
        "spam_data = [\n",
        "    # Spam examples (label=1)\n",
        "    (\"Ch√∫c m·ª´ng! B·∫°n ƒë√£ tr√∫ng th∆∞·ªüng 100 tri·ªáu. Click ngay!\", 1),\n",
        "    (\"Ki·∫øm ti·ªÅn online d·ªÖ d√†ng, thu nh·∫≠p 50tr/th√°ng\", 1),\n",
        "    (\"Gi·∫£m c√¢n nhanh ch√≥ng kh√¥ng c·∫ßn t·∫≠p luy·ªán\", 1),\n",
        "    (\"Free gift! Claim your prize now!\", 1),\n",
        "    (\"Khuy·∫øn m√£i ƒë·∫∑c bi·ªát ch·ªâ h√¥m nay, gi·∫£m 90%!\", 1),\n",
        "    (\"You have won a lottery! Click here!\", 1),\n",
        "    # Ham examples (label=0)\n",
        "    (\"Cu·ªôc h·ªçp ƒë∆∞·ª£c l√™n l·ªãch v√†o ng√†y mai l√∫c 10 gi·ªù\", 0),\n",
        "    (\"Vui l√≤ng xem x√©t t√†i li·ªáu ƒë√≠nh k√®m\", 0),\n",
        "    (\"C·∫£m ∆°n email c·ªßa b·∫°n v·ªÅ d·ª± √°n\", 0),\n",
        "    (\"Meeting scheduled for tomorrow at 10 AM\", 0),\n",
        "    (\"Please review the attached document\", 0),\n",
        "    (\"Thank you for your email regarding the project\", 0),\n",
        "]\n",
        "\n",
        "# Sentiment dataset\n",
        "sentiment_data = [\n",
        "    # Positive (label=2)\n",
        "    (\"C·∫£m ∆°n b·∫°n r·∫•t nhi·ªÅu! D·ªãch v·ª• tuy·ªát v·ªùi!\", 2),\n",
        "    (\"T√¥i r·∫•t h√†i l√≤ng v·ªõi s·∫£n ph·∫©m n√†y\", 2),\n",
        "    (\"Great job! Thanks for your help!\", 2),\n",
        "    # Neutral (label=1)\n",
        "    (\"T√¥i mu·ªën h·ªèi v·ªÅ ƒë∆°n h√†ng c·ªßa m√¨nh\", 1),\n",
        "    (\"Xin cho t√¥i bi·∫øt th√™m th√¥ng tin\", 1),\n",
        "    (\"I would like to inquire about my order\", 1),\n",
        "    # Negative (label=0)\n",
        "    (\"D·ªãch v·ª• r·∫•t t·ªá, t√¥i r·∫•t th·∫•t v·ªçng\", 0),\n",
        "    (\"S·∫£n ph·∫©m b·ªã l·ªói, y√™u c·∫ßu ho√†n ti·ªÅn\", 0),\n",
        "    (\"This is terrible service. I want a refund.\", 0),\n",
        "]\n",
        "\n",
        "# Category dataset\n",
        "category_data = [\n",
        "    # Primary (label=0)\n",
        "    (\"Cu·ªôc h·ªçp v√†o l√∫c 3 gi·ªù chi·ªÅu nay\", 0),\n",
        "    (\"Please send me the report by EOD\", 0),\n",
        "    # Important (label=1)\n",
        "    (\"KH·∫®N C·∫§P: C·∫ßn ph·∫£n h·ªìi ngay l·∫≠p t·ª©c\", 1),\n",
        "    (\"URGENT: Your account needs verification\", 1),\n",
        "    # Social (label=2)\n",
        "    (\"Ai ƒë√≥ ƒë√£ th√≠ch b√†i vi·∫øt c·ªßa b·∫°n\", 2),\n",
        "    (\"You have a new friend request\", 2),\n",
        "    # Promotions (label=3)\n",
        "    (\"Gi·∫£m gi√° 50% t·∫•t c·∫£ s·∫£n ph·∫©m\", 3),\n",
        "    (\"Summer sale - 50% off everything!\", 3),\n",
        "    # Updates (label=4)\n",
        "    (\"ƒê∆°n h√†ng c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c giao\", 4),\n",
        "    (\"Your package has been delivered\", 4),\n",
        "]\n",
        "\n",
        "# Convert to DataFrames\n",
        "df_spam = pd.DataFrame(spam_data, columns=['text', 'label'])\n",
        "df_sentiment = pd.DataFrame(sentiment_data, columns=['text', 'label'])\n",
        "df_category = pd.DataFrame(category_data, columns=['text', 'label'])\n",
        "\n",
        "print(f'Spam dataset: {len(df_spam)} samples')\n",
        "print(f'Sentiment dataset: {len(df_sentiment)} samples')\n",
        "print(f'Category dataset: {len(df_category)} samples')"
      ],
      "metadata": {
        "id": "sample_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload your own dataset (optional)\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# df_spam = pd.read_csv('your_spam_data.csv')"
      ],
      "metadata": {
        "id": "upload_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Tokenize Data"
      ],
      "metadata": {
        "id": "tokenize"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples['text'],\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=256\n",
        "    )\n",
        "\n",
        "def prepare_dataset(df):\n",
        "    \"\"\"Convert DataFrame to HuggingFace Dataset\"\"\"\n",
        "    dataset = Dataset.from_pandas(df)\n",
        "    tokenized = dataset.map(tokenize_function, batched=True)\n",
        "    return tokenized\n",
        "\n",
        "# Prepare datasets\n",
        "spam_dataset = prepare_dataset(df_spam)\n",
        "sentiment_dataset = prepare_dataset(df_sentiment)\n",
        "category_dataset = prepare_dataset(df_category)\n",
        "\n",
        "print('Datasets prepared!')"
      ],
      "metadata": {
        "id": "prepare_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Train Spam Detection Model"
      ],
      "metadata": {
        "id": "train_spam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(dataset, num_labels, output_dir, epochs=3):\n",
        "    \"\"\"Train a PhoBERT classification model\"\"\"\n",
        "    \n",
        "    # Load model\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=num_labels\n",
        "    )\n",
        "    \n",
        "    # Split dataset\n",
        "    split = dataset.train_test_split(test_size=0.2)\n",
        "    \n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        warmup_steps=100,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=f'{output_dir}/logs',\n",
        "        logging_steps=10,\n",
        "        eval_strategy='epoch',\n",
        "        save_strategy='epoch',\n",
        "        load_best_model_at_end=True,\n",
        "    )\n",
        "    \n",
        "    # Data collator\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "    \n",
        "    # Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=split['train'],\n",
        "        eval_dataset=split['test'],\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "    \n",
        "    # Train\n",
        "    trainer.train()\n",
        "    \n",
        "    # Save model\n",
        "    model.save_pretrained(output_dir)\n",
        "    \n",
        "    return model, trainer\n",
        "\n",
        "# Train spam model (2 classes: ham=0, spam=1)\n",
        "print('Training Spam Detection Model...')\n",
        "spam_model, spam_trainer = train_model(\n",
        "    spam_dataset, \n",
        "    num_labels=2, \n",
        "    output_dir='./spam_model',\n",
        "    epochs=3\n",
        ")\n",
        "print('Spam model trained!')"
      ],
      "metadata": {
        "id": "train_spam_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Train Sentiment Analysis Model"
      ],
      "metadata": {
        "id": "train_sentiment_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train sentiment model (3 classes: negative=0, neutral=1, positive=2)\n",
        "print('Training Sentiment Analysis Model...')\n",
        "sentiment_model, sentiment_trainer = train_model(\n",
        "    sentiment_dataset,\n",
        "    num_labels=3,\n",
        "    output_dir='./sentiment_model',\n",
        "    epochs=3\n",
        ")\n",
        "print('Sentiment model trained!')"
      ],
      "metadata": {
        "id": "train_sentiment"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Train Category Classification Model"
      ],
      "metadata": {
        "id": "train_category_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train category model (5 classes)\n",
        "print('Training Category Classification Model...')\n",
        "category_model, category_trainer = train_model(\n",
        "    category_dataset,\n",
        "    num_labels=5,\n",
        "    output_dir='./category_model',\n",
        "    epochs=3\n",
        ")\n",
        "print('Category model trained!')"
      ],
      "metadata": {
        "id": "train_category"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Save Tokenizer"
      ],
      "metadata": {
        "id": "save_tokenizer_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save tokenizer\n",
        "tokenizer.save_pretrained('./tokenizer')\n",
        "print('Tokenizer saved!')"
      ],
      "metadata": {
        "id": "save_tokenizer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Test Models"
      ],
      "metadata": {
        "id": "test_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, text, label_map):\n",
        "    \"\"\"Test a single prediction\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=256)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        probs = torch.softmax(outputs.logits, dim=-1)\n",
        "        pred_class = torch.argmax(probs, dim=-1).item()\n",
        "        confidence = probs[0][pred_class].item()\n",
        "    \n",
        "    return label_map[pred_class], confidence\n",
        "\n",
        "# Test spam detection\n",
        "spam_labels = {0: 'Ham', 1: 'Spam'}\n",
        "test_texts = [\n",
        "    \"Cu·ªôc h·ªçp v√†o 3 gi·ªù chi·ªÅu\",\n",
        "    \"B·∫°n ƒë√£ tr√∫ng th∆∞·ªüng 1 t·ª∑ ƒë·ªìng!\",\n",
        "]\n",
        "print('\\n=== Spam Detection Test ===')\n",
        "for text in test_texts:\n",
        "    label, conf = test_model(spam_model, text, spam_labels)\n",
        "    print(f'Text: \"{text}\"')\n",
        "    print(f'Prediction: {label} (confidence: {conf:.2%})\\n')\n",
        "\n",
        "# Test sentiment\n",
        "sentiment_labels = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
        "test_texts = [\n",
        "    \"C·∫£m ∆°n b·∫°n r·∫•t nhi·ªÅu!\",\n",
        "    \"D·ªãch v·ª• r·∫•t t·ªá\",\n",
        "]\n",
        "print('=== Sentiment Analysis Test ===')\n",
        "for text in test_texts:\n",
        "    label, conf = test_model(sentiment_model, text, sentiment_labels)\n",
        "    print(f'Text: \"{text}\"')\n",
        "    print(f'Prediction: {label} (confidence: {conf:.2%})\\n')"
      ],
      "metadata": {
        "id": "test_models"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Download Models"
      ],
      "metadata": {
        "id": "download_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip and download all models\n",
        "!zip -r models.zip spam_model sentiment_model category_model tokenizer\n",
        "\n",
        "from google.colab import files\n",
        "files.download('models.zip')\n",
        "\n",
        "print('\\n‚úÖ Download complete!')\n",
        "print('Extract models.zip and copy to PhoBERT-Service/models/ directory')"
      ],
      "metadata": {
        "id": "download_models"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Next Steps\n",
        "\n",
        "1. Download file `models.zip`\n",
        "2. Extract v√†o th∆∞ m·ª•c `PhoBERT-Service/models/`\n",
        "3. C·∫•u tr√∫c th∆∞ m·ª•c:\n",
        "   ```\n",
        "   PhoBERT-Service/models/\n",
        "   ‚îú‚îÄ‚îÄ spam_model/\n",
        "   ‚îú‚îÄ‚îÄ sentiment_model/\n",
        "   ‚îú‚îÄ‚îÄ category_model/\n",
        "   ‚îî‚îÄ‚îÄ tokenizer/\n",
        "   ```\n",
        "4. Ch·∫°y PhoBERT service:\n",
        "   ```bash\n",
        "   cd PhoBERT-Service\n",
        "   pip install -r requirements.txt\n",
        "   uvicorn app.main:app --host 0.0.0.0 --port 8000\n",
        "   ```\n",
        "5. C·∫≠p nh·∫≠t Email-System-Server `.env`:\n",
        "   ```\n",
        "   PHOBERT_URL=http://localhost:8000\n",
        "   ```"
      ],
      "metadata": {
        "id": "next_steps"
      }
    }
  ]
}
