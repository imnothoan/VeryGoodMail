{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üáªüá≥ VeryGoodMail - PhoBERT Email Classification Training\n",
        "\n",
        "Notebook n√†y gi√∫p b·∫°n train c√°c model PhoBERT cho:\n",
        "- **Spam Detection**: Ph√°t hi·ªán email spam\n",
        "- **Sentiment Analysis**: Ph√¢n t√≠ch c·∫£m x√∫c\n",
        "- **Category Classification**: Ph√¢n lo·∫°i email\n",
        "\n",
        "¬© 2025 VeryGoodMail by Ho√†n"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup Environment"
      ],
      "metadata": {
        "id": "setup"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install transformers torch datasets scikit-learn pandas underthesea -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from datasets import Dataset\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check GPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using device: {device}')\n",
        "if device == 'cuda':\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load PhoBERT Tokenizer"
      ],
      "metadata": {
        "id": "tokenizer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load PhoBERT tokenizer\n",
        "model_name = \"vinai/phobert-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "print(f'Loaded tokenizer: {model_name}')"
      ],
      "metadata": {
        "id": "load_tokenizer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload datasets t·ª´ local (ch·∫°y cell n√†y ƒë·ªÉ up c√°c file CSV/rar em t·∫£i)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # Ch·ªçn file t·ª´ m√°y t√≠nh: vietnamese_spam_post.csv, data.csv, viet_text_class.csv, Train_Full.rar, Test_Full.rar (n·∫øu d√πng VNTC)\n",
        "\n",
        "# N·∫øu c√≥ rar cho VNTC, unrar\n",
        "!unrar x Train_Full.rar  # Adjust t√™n file n·∫øu kh√°c\n",
        "!unrar x Test_Full.rar"
      ],
      "metadata": {
        "id": "xVf3cuGPnjkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Prepare Dataset\n",
        "\n",
        "Upload dataset c·ªßa b·∫°n ho·∫∑c s·ª≠ d·ª•ng sample data"
      ],
      "metadata": {
        "id": "data"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data - Thay th·∫ø b·∫±ng dataset c·ªßa b·∫°n\n",
        "# Format: text, label\n",
        "\n",
        "# Load Spam Dataset 1: ViSpamReviews t·ª´ Hugging Face (kh√¥ng c·∫ßn t·∫£i file)\n",
        "from datasets import load_dataset\n",
        "ds_spam1 = load_dataset(\"SEACrowd/vispamreviews\")\n",
        "df_spam1 = pd.DataFrame(ds_spam1['train'])  # Ho·∫∑c concat train/test n·∫øu c√≥\n",
        "df_spam1 = df_spam1[['review', 'label']]  # C·ªôt text v√† label (adjust n·∫øu kh√°c)\n",
        "df_spam1.columns = ['text', 'label']\n",
        "df_spam1['label'] = df_spam1['label'].astype(int)  # 0 non-spam, 1 spam\n",
        "\n",
        "# Load Spam Dataset 2: Vietnamese Spam Post t·ª´ Kaggle (file ƒë√£ up)\n",
        "df_spam2 = pd.read_csv('vietnamese_spam_post.csv')  # Adjust t√™n file n·∫øu kh√°c\n",
        "df_spam2 = df_spam2[['post', 'label']]  # Adjust c·ªôt n·∫øu kh√°c (v√≠ d·ª• 'text' thay 'post')\n",
        "df_spam2.columns = ['text', 'label']\n",
        "df_spam2['label'] = df_spam2['label'].astype(int)\n",
        "\n",
        "# Merge hai dataset\n",
        "df_spam = pd.concat([df_spam1, df_spam2], ignore_index=True)\n",
        "print(f'Spam dataset merged: {len(df_spam)} samples')\n",
        "\n",
        "# Load Sentiment Dataset 1: UIT-VSFC t·ª´ Hugging Face\n",
        "from datasets import load_dataset\n",
        "ds_sent1 = load_dataset(\"uitnlp/vietnamese_students_feedback\")\n",
        "df_sent1_train = pd.DataFrame(ds_sent1['train'])[['sentence', 'sentiment']]\n",
        "df_sent1_val = pd.DataFrame(ds_sent1['validation'])[['sentence', 'sentiment']]\n",
        "df_sent1_test = pd.DataFrame(ds_sent1['test'])[['sentence', 'sentiment']]\n",
        "df_sent1 = pd.concat([df_sent1_train, df_sent1_val, df_sent1_test], ignore_index=True)\n",
        "df_sent1.columns = ['text', 'label']\n",
        "df_sent1['label'] = df_sent1['label'].astype(int)  # 0 negative, 1 neutral, 2 positive\n",
        "\n",
        "# Load Sentiment Dataset 2: Vietnamese Sentiment Analyst t·ª´ Kaggle (file ƒë√£ up)\n",
        "df_sent2 = pd.read_csv('data.csv')  # Adjust t√™n file n·∫øu kh√°c\n",
        "df_sent2 = df_sent2[['comment', 'label']]  # Adjust c·ªôt n·∫øu kh√°c\n",
        "df_sent2.columns = ['text', 'label']\n",
        "# N·∫øu label l√† string, map: uncomment d√≤ng d∆∞·ªõi\n",
        "# df_sent2['label'] = df_sent2['label'].map({'negative': 0, 'neutral': 1, 'positive': 2})\n",
        "\n",
        "# Merge\n",
        "df_sentiment = pd.concat([df_sent1, df_sent2], ignore_index=True)\n",
        "df_sentiment['label'] = df_sentiment['label'].astype(int)\n",
        "print(f'Sentiment dataset merged: {len(df_sentiment)} samples')\n",
        "\n",
        "# Load Category Dataset 1: Vietnamese Text Classification t·ª´ Kaggle (file ƒë√£ up)\n",
        "df_cat1 = pd.read_csv('viet_text_class.csv')  # Adjust t√™n file n·∫øu kh√°c\n",
        "df_cat1 = df_cat1[['text', 'label']]  # Assume c·ªôt s·∫µn\n",
        "df_cat1['label'] = df_cat1['label'].astype(int)  # Labels 0-n (kho·∫£ng 10 class)\n",
        "\n",
        "# Load Category Dataset 2: VNTC (n·∫øu em t·∫£i v√† unrar folders)\n",
        "# N·∫øu skip VNTC, comment ph·∫ßn n√†y v√† d√πng df_category = df_cat1\n",
        "import os\n",
        "train_dir = 'Train_Full/'  # Path ƒë·∫øn folder unrar\n",
        "test_dir = 'Test_Full/'  # N·∫øu c√≥\n",
        "categories = os.listdir(train_dir)  # C√°c folder category\n",
        "df_cat2 = pd.DataFrame(columns=['text', 'label'])\n",
        "for idx, cat in enumerate(categories):\n",
        "    cat_dir = os.path.join(train_dir, cat)\n",
        "    for file in os.listdir(cat_dir):\n",
        "        if file.endswith('.txt'):\n",
        "            with open(os.path.join(cat_dir, file), 'r', encoding='utf-16') as f:  # VNTC d√πng utf-16\n",
        "                text = f.read().strip()\n",
        "            df_cat2 = pd.concat([df_cat2, pd.DataFrame({'text': [text], 'label': [idx]})], ignore_index=True)\n",
        "# T∆∞∆°ng t·ª± cho test_dir n·∫øu mu·ªën concat\n",
        "\n",
        "# Merge\n",
        "df_category = pd.concat([df_cat1, df_cat2], ignore_index=True) if 'df_cat2' in locals() else df_cat1\n",
        "df_category['label'] = df_category['label'].astype(int)\n",
        "# N·∫øu mu·ªën gi·ªØ 5 class nh∆∞ g·ªëc, map random: df_category['label'] = df_category['label'] % 5\n",
        "# Adjust num_labels trong train_model category th√†nh len(df_category['label'].unique())\n",
        "\n",
        "print(f'Category dataset merged: {len(df_category)} samples')\n",
        "\n",
        "# Convert to DataFrames\n",
        "df_spam = pd.DataFrame(spam_data, columns=['text', 'label'])\n",
        "df_sentiment = pd.DataFrame(sentiment_data, columns=['text', 'label'])\n",
        "df_category = pd.DataFrame(category_data, columns=['text', 'label'])\n",
        "\n",
        "print(f'Spam dataset: {len(df_spam)} samples')\n",
        "print(f'Sentiment dataset: {len(df_sentiment)} samples')\n",
        "print(f'Category dataset: {len(df_category)} samples')"
      ],
      "metadata": {
        "id": "sample_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload your own dataset (optional)\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# df_spam = pd.read_csv('your_spam_data.csv')"
      ],
      "metadata": {
        "id": "upload_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Tokenize Data"
      ],
      "metadata": {
        "id": "tokenize"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples['text'],\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=256\n",
        "    )\n",
        "\n",
        "def prepare_dataset(df):\n",
        "    \"\"\"Convert DataFrame to HuggingFace Dataset\"\"\"\n",
        "    dataset = Dataset.from_pandas(df)\n",
        "    tokenized = dataset.map(tokenize_function, batched=True)\n",
        "    return tokenized\n",
        "\n",
        "# Prepare datasets\n",
        "spam_dataset = prepare_dataset(df_spam)\n",
        "sentiment_dataset = prepare_dataset(df_sentiment)\n",
        "category_dataset = prepare_dataset(df_category)\n",
        "\n",
        "print('Datasets prepared!')"
      ],
      "metadata": {
        "id": "prepare_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Train Spam Detection Model"
      ],
      "metadata": {
        "id": "train_spam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(dataset, num_labels, output_dir, epochs=3):\n",
        "    \"\"\"Train a PhoBERT classification model\"\"\"\n",
        "\n",
        "    # Load model\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=num_labels\n",
        "    )\n",
        "\n",
        "    # Split dataset\n",
        "    split = dataset.train_test_split(test_size=0.2)\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        warmup_steps=100,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=f'{output_dir}/logs',\n",
        "        logging_steps=10,\n",
        "        eval_strategy='epoch',\n",
        "        save_strategy='epoch',\n",
        "        load_best_model_at_end=True,\n",
        "    )\n",
        "\n",
        "    # Data collator\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    # Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=split['train'],\n",
        "        eval_dataset=split['test'],\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    trainer.train()\n",
        "\n",
        "    # Save model\n",
        "    model.save_pretrained(output_dir)\n",
        "\n",
        "    return model, trainer\n",
        "\n",
        "# Train spam model (2 classes: ham=0, spam=1)\n",
        "print('Training Spam Detection Model...')\n",
        "spam_model, spam_trainer = train_model(\n",
        "    spam_dataset,\n",
        "    num_labels=2,\n",
        "    output_dir='./spam_model',\n",
        "    epochs=3\n",
        ")\n",
        "print('Spam model trained!')"
      ],
      "metadata": {
        "id": "train_spam_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Train Sentiment Analysis Model"
      ],
      "metadata": {
        "id": "train_sentiment_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train sentiment model (3 classes: negative=0, neutral=1, positive=2)\n",
        "print('Training Sentiment Analysis Model...')\n",
        "sentiment_model, sentiment_trainer = train_model(\n",
        "    sentiment_dataset,\n",
        "    num_labels=3,\n",
        "    output_dir='./sentiment_model',\n",
        "    epochs=3\n",
        ")\n",
        "print('Sentiment model trained!')"
      ],
      "metadata": {
        "id": "train_sentiment"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Train Category Classification Model"
      ],
      "metadata": {
        "id": "train_category_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train category model (5 classes)\n",
        "print('Training Category Classification Model...')\n",
        "category_model, category_trainer = train_model(\n",
        "    category_dataset,\n",
        "    num_labels=5,\n",
        "    output_dir='./category_model',\n",
        "    epochs=3\n",
        ")\n",
        "print('Category model trained!')"
      ],
      "metadata": {
        "id": "train_category"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Save Tokenizer"
      ],
      "metadata": {
        "id": "save_tokenizer_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save tokenizer\n",
        "tokenizer.save_pretrained('./tokenizer')\n",
        "print('Tokenizer saved!')"
      ],
      "metadata": {
        "id": "save_tokenizer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Test Models"
      ],
      "metadata": {
        "id": "test_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, text, label_map):\n",
        "    \"\"\"Test a single prediction\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=256)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        probs = torch.softmax(outputs.logits, dim=-1)\n",
        "        pred_class = torch.argmax(probs, dim=-1).item()\n",
        "        confidence = probs[0][pred_class].item()\n",
        "\n",
        "    return label_map[pred_class], confidence\n",
        "\n",
        "# Test spam detection\n",
        "spam_labels = {0: 'Ham', 1: 'Spam'}\n",
        "test_texts = [\n",
        "    \"Cu·ªôc h·ªçp v√†o 3 gi·ªù chi·ªÅu\",\n",
        "    \"B·∫°n ƒë√£ tr√∫ng th∆∞·ªüng 1 t·ª∑ ƒë·ªìng!\",\n",
        "]\n",
        "print('\\n=== Spam Detection Test ===')\n",
        "for text in test_texts:\n",
        "    label, conf = test_model(spam_model, text, spam_labels)\n",
        "    print(f'Text: \"{text}\"')\n",
        "    print(f'Prediction: {label} (confidence: {conf:.2%})\\n')\n",
        "\n",
        "# Test sentiment\n",
        "sentiment_labels = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
        "test_texts = [\n",
        "    \"C·∫£m ∆°n b·∫°n r·∫•t nhi·ªÅu!\",\n",
        "    \"D·ªãch v·ª• r·∫•t t·ªá\",\n",
        "]\n",
        "print('=== Sentiment Analysis Test ===')\n",
        "for text in test_texts:\n",
        "    label, conf = test_model(sentiment_model, text, sentiment_labels)\n",
        "    print(f'Text: \"{text}\"')\n",
        "    print(f'Prediction: {label} (confidence: {conf:.2%})\\n')"
      ],
      "metadata": {
        "id": "test_models"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Download Models"
      ],
      "metadata": {
        "id": "download_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip and download all models\n",
        "!zip -r models.zip spam_model sentiment_model category_model tokenizer\n",
        "\n",
        "from google.colab import files\n",
        "files.download('models.zip')\n",
        "\n",
        "print('\\n‚úÖ Download complete!')\n",
        "print('Extract models.zip and copy to PhoBERT-Service/models/ directory')"
      ],
      "metadata": {
        "id": "download_models"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Next Steps\n",
        "\n",
        "1. Download file `models.zip`\n",
        "2. Extract v√†o th∆∞ m·ª•c `PhoBERT-Service/models/`\n",
        "3. C·∫•u tr√∫c th∆∞ m·ª•c:\n",
        "   ```\n",
        "   PhoBERT-Service/models/\n",
        "   ‚îú‚îÄ‚îÄ spam_model/\n",
        "   ‚îú‚îÄ‚îÄ sentiment_model/\n",
        "   ‚îú‚îÄ‚îÄ category_model/\n",
        "   ‚îî‚îÄ‚îÄ tokenizer/\n",
        "   ```\n",
        "4. Ch·∫°y PhoBERT service:\n",
        "   ```bash\n",
        "   cd PhoBERT-Service\n",
        "   pip install -r requirements.txt\n",
        "   uvicorn app.main:app --host 0.0.0.0 --port 8000\n",
        "   ```\n",
        "5. C·∫≠p nh·∫≠t Email-System-Server `.env`:\n",
        "   ```\n",
        "   PHOBERT_URL=http://localhost:8000\n",
        "   ```"
      ],
      "metadata": {
        "id": "next_steps"
      }
    }
  ]
}